= OpenAI Component
:doctitle: OpenAI
:shortname: openai
:artifactid: camel-openai
:description: OpenAI endpoint for chat completion and embeddings.
:since: 4.17
:supportlevel: Stable
:tabs-sync-option:
:component-header: Only producer is supported
//Manually maintained attributes
:group: AI
:camel-spring-boot-name: openai

*Since Camel {since}*

*{component-header}*

The OpenAI component provides integration with OpenAI and OpenAI-compatible APIs for chat completion and text embeddings using the official openai-java SDK.

Maven users will need to add the following dependency to their `pom.xml` for this component:

[source,xml]
----
<dependency>
    <groupId>org.apache.camel</groupId>
    <artifactId>camel-openai</artifactId>
    <version>x.x.x</version>
    <!-- use the same version as your Camel core version -->
</dependency>
----

== URI Format

[source]
----
openai:operation[?options]
----

Supported operations:

* `chat-completion` - Generate chat completions using language models
* `embeddings` - Generate vector embeddings from text for semantic search and RAG applications
* `tool-execution` - Execute MCP tool calls from a stored chat completion response (used in manual tool loops)

// component options: START
include::partial$component-configure-options.adoc[]
include::partial$component-endpoint-options.adoc[]
include::partial$component-endpoint-headers.adoc[]
// component options: END

== Usage

=== Authentication

Set `baseUrl` to your providers endpoint (default: `https://api.openai.com/v1`).

API key resolution order:

- Endpoint `apiKey`
- Component `apiKey`
- Environment variable `OPENAI_API_KEY`
- System property `openai.api.key`

[NOTE]
====
The API key can be omitted if using OpenAI-compatible providers that don't require authentication (e.g., some local LLM servers).
====

==== OAuth Authentication

When using an identity provider (e.g., Azure AD for Azure OpenAI), set the `oauthProfile` parameter to acquire an access token via the OAuth 2.0 Client Credentials grant. The token is used in place of the API key. This requires `camel-oauth` on the classpath.

[source,properties]
----
camel.oauth.azure.client-id=my-client
camel.oauth.azure.client-secret=my-secret
camel.oauth.azure.token-endpoint=https://login.microsoftonline.com/tenant/oauth2/v2.0/token
camel.oauth.azure.scope=https://cognitiveservices.azure.com/.default
----

[source,java]
----
from("direct:chat")
    .to("openai:chat-completion?model=gpt-4&oauthProfile=azure");
----

MCP servers can also use OAuth independently via per-server `oauthProfile`:

[source,java]
----
from("direct:chat")
    .to("openai:chat-completion?model=gpt-4"
        + "&oauthProfile=azure"
        + "&mcpServer.tools.transportType=streamableHttp"
        + "&mcpServer.tools.url=https://mcp.internal/mcp"
        + "&mcpServer.tools.oauthProfile=keycloak");
----

=== Basic Chat Completion with String Input

[tabs]
====
Java::
+
[source,java]
----
from("direct:chat")
    .setBody(constant("What is Apache Camel?"))
    .to("openai:chat-completion")
    .log("Response: ${body}");
----

YAML::
+
[source,yaml]
----
- route:
    from:
      uri: direct:chat
      steps:
        - to:
            uri: openai:chat-completion
            parameters:
              userMessage: What is Apache Camel?
        - log: "Response: ${body}"
----
====

=== File-Backed Prompt with Text File

.Usage example:
[source,java]
----
from("file:prompts?noop=true")
    .to("openai:chat-completion")
    .log("Response: ${body}");
----



=== Image File Input with Vision Model

.Usage example:
[source,java]
----
from("file:images?noop=true")
    .to("openai:chat-completion?model=gpt-4.1-mini?userMessage=Describe what you see in this image")
    .log("Response: ${body}");
----

[NOTE]
====
When using image files, the userMessage is required. Supported image formats are detected by MIME type (e.g., `image/png`, `image/jpeg`, `image/gif`, `image/webp`).
====

=== Streaming Response

When `streaming=true`, the component returns an `Iterator<ChatCompletionChunk>` in the message body. You can consume this iterator using Camel's streaming EIPs or process it directly:

.Usage example:
[source,yaml]
----
- route:
    id: route-1145
    from:
      id: from-1972
      uri: timer
      parameters:
        repeatCount: 1
        timerName: timer
      steps:
        - to:
            id: to-1301
            uri: openai:chat-completion
            parameters:
              userMessage: In one sentence, what is Apache Camel?
              streaming: true
        - split:
            id: split-3196
            steps:
              - marshal:
                  id: marshal-3773
                  json:
                    library: Jackson
              - log:
                  id: log-6722
                  message: ${body}
            simple:
              expression: ${body}
            streaming: true
----

=== Structured Output with outputClass

.When `outputClass` is set, the model is instructed to produce JSON matching the given class, but the component returns the raw String. Deserialize the body yourself (e.g., with Camel's Jackson) if you need a typed object.

.Usage example:
[source,java]
----
public class Person {
    public String name;
    public int age;
    public String occupation;
}

from("direct:structured")
    .setBody(constant("Generate a person profile for a software engineer"))
    .to("openai:chat-completion?baseUrl=https://api.openai.com/v1&outputClass=com.example.Person")
    .log("Structured response: ${body}");
----

=== Structured Output with JSON Schema

The `jsonSchema` option instructs the model to return JSON that conforms to the provided schema. The response will be valid JSON but is not automatically validated against the schema:

.Usage example:
[source,java]
----
from("direct:json-schema")
    .setBody(constant("Create a product description"))
    .setHeader("CamelOpenAIJsonSchema", constant("{\"type\":\"object\",\"properties\":{\"name\":{\"type\":\"string\"},\"price\":{\"type\":\"number\"}}}"))
    .to("openai:chat-completion")
    .log("JSON response: ${body}");
----

You can also load the schema from a resource file:

.Usage example:
[source,java]
----
from("direct:json-schema-resource")
    .setBody(constant("Create a product description"))
    .to("openai:chat-completion?jsonSchema=resource:classpath:schemas/product.schema.json")
    .log("JSON response: ${body}");
----

[NOTE]
====
For full schema validation, integrate with the `camel-json-validator` component after receiving the response.
====

=== Conversation Memory (Per Exchange)

.Usage example:
[source,java]
----
from("direct:conversation")
    .setBody(constant("My name is Alice"))
    .to("openai:chat-completion?conversationMemory=true")
    .log("First response: ${body}")
    .setBody(constant("What is my name?"))
    .to("openai:chat-completion?conversationMemory=true")
    .log("Second response: ${body}"); // Will remember "Alice"
----

=== Using Third-Party or Local OpenAI-Compatible Endpoint

.Usage example:
[source,java]
----
from("direct:local")
    .setBody(constant("Hello from local LLM"))
    .to("openai:chat-completion?baseUrl=http://localhost:1234/v1&model=local-model")
    .log("${body}");
----

== Input Handling

The component accepts the following types of input in the message body:

1. *String*: The prompt text is taken directly from the body
2. *File*: Used for file-based prompts. The component handles two types of files:
   * *Text files* (MIME type starting with `text/`): The file content is read and used as the prompt. If userMessage endpoint option or `CamelOpenAIUserMessage` is set, it overrides the file content
   * *Image files* (MIME type starting with `image/`): The file is encoded as a base64 data URL and sent to vision-capable models. The userMessage is **required** when using image files

[NOTE]
====
When using `File` input, the component uses `Files.probeContentType()` to detect the file type. Ensure your system has proper MIME type detection configured.
====

== Output Handling

=== Default Mode
The full model response is returned as a String in the message body.

=== Streaming Mode
When `streaming=true`, the message body contains an `Iterator<ChatCompletionChunk>` suitable for Camel streaming EIPs (such as `split()` with `streaming()`).

IMPORTANT:

* Conversation memory is **not** automatically updated for streaming responses (only for non-streaming responses)

=== Structured Outputs

==== Using outputClass
The model is instructed to return JSON matching the specified class, but the response body remains a String.

==== Using jsonSchema
The `jsonSchema` option instructs the model to return JSON conforming to the provided schema. The response will be valid JSON but is not automatically validated against the schema. For full schema validation, integrate with the `camel-json-validator` component after receiving the response.

The JSON schema must be a valid JSON object. Invalid schema strings will result in an `IllegalArgumentException`.

== Conversation Memory

When `conversationMemory=true`, the component maintains conversation history in the `CamelOpenAIConversationHistory` exchange property (configurable via `conversationHistoryProperty` option). This history is scoped to a single Exchange and allows multi-turn conversations within a route.

IMPORTANT:

* Conversation history is automatically updated with each assistant response for **non-streaming** responses only
* The history is stored as a `List<ChatCompletionMessageParam>` in the Exchange property
* The history persists across multiple calls to the endpoint within the same Exchange
* You can manually set the `CamelOpenAIConversationHistory` exchange property to provide custom conversation context

Example of manual conversation history:

.Usage example:
[source,java]
----
List<ChatCompletionMessageParam> history = new ArrayList<>();
history.add(ChatCompletionMessageParam.ofUser(/* ... */));
history.add(ChatCompletionMessageParam.ofAssistant(/* ... */));

from("direct:with-history")
    .setBody(constant("Continue the conversation"))
    .setProperty("CamelOpenAIConversationHistory", constant(history))
    .to("openai:chat-completion?conversationMemory=true")
    .log("${body}");
----

== Compatibility

This component works with any OpenAI API-compatible endpoint by setting the `baseUrl` parameter. This includes:

- OpenAI official API (`https://api.openai.com/v1`)
- Local LLM servers (e.g., Ollama, LM Studio, LocalAI)
- Third-party OpenAI-compatible providers

[NOTE]
====
When using local or third-party providers, ensure they support the chat completions and/or embeddings API endpoint format. Some providers may have different authentication requirements or API variations.
====

=== Embedding Models by Provider

[cols="1,2,1"]
|===
| Provider | Recommended Model | Dimensions

| OpenAI | `text-embedding-3-small` | 1536 (reducible to 256, 512, 1024)
| OpenAI | `text-embedding-3-large` | 3072 (reducible)
| Ollama | `nomic-embed-text` | 768
| Ollama | `mxbai-embed-large` | 1024
| Mistral | `mistral-embed` | 1024
|===

.Example using Ollama for local embeddings:
[source,yaml]
----
- to:
    uri: openai:embeddings
    parameters:
      baseUrl: http://localhost:11434/v1
      embeddingModel: nomic-embed-text
----

== Embeddings Operation

The `embeddings` operation generates vector embeddings from text, which can be used for semantic search, similarity comparison, and RAG (Retrieval-Augmented Generation) applications.

=== Basic Embedding

[tabs]
====
Java::
+
[source,java]
----
from("direct:embed")
    .setBody(constant("What is Apache Camel?"))
    .to("openai:embeddings?embeddingModel=nomic-embed-text")
----

YAML::
+
[source,yaml]
----
- route:
    from:
      uri: direct:embed
      steps:
        - to:
            uri: openai:embeddings
            parameters:
              embeddingModel: nomic-embed-text
----
====

The response body is the embedding vector data:

* Single input: `List<Float>` (a single embedding vector)
* Batch input: `List<List<Float>>` (one embedding vector per input string)

Additional metadata (model, token usage, vector size, count) is exposed via headers (see `OpenAIConstants`).

=== Batch Embedding

You can embed multiple texts in a single request by passing a `List<String>`:

[source,java]
----
from("direct:batch-embed")
    .setBody(constant(List.of("First text", "Second text", "Third text")))
    .to("openai:embeddings?embeddingModel=nomic-embed-text")
    .log("Generated ${header.CamelOpenAIEmbeddingCount} embeddings");
----

=== Direct Vector Database Integration

For single-input requests, the component returns a raw `List<Float>` embedding vector, enabling direct chaining to vector database components.

==== PostgreSQL + pgvector (Recommended)

[source,yaml]
----
# Index documents in PostgreSQL with pgvector
- route:
    from:
      uri: direct:index
      steps:
        - setVariable:
            name: text
            simple: "${body}"
        - to:
            uri: openai:embeddings
            parameters:
              embeddingModel: nomic-embed-text
        - setVariable:
            name: embedding
            simple: "${body.toString()}"
        - to:
            uri: sql:INSERT INTO documents (content, embedding) VALUES (:#text, :#embedding::vector)
----

==== Alternative: Dedicated Vector Databases

For specialized vector workloads, you can also use `camel-qdrant`, `camel-weaviate`, `camel-milvus`, or `camel-pinecone`:

=== Similarity Calculation

The component can automatically calculate cosine similarity when a reference embedding is provided:

[source,java]
----
List<Float> referenceEmbedding = /* previously computed embedding */;

from("direct:compare")
    .setBody(constant("New text to compare"))
    .setHeader("CamelOpenAIReferenceEmbedding", constant(referenceEmbedding))
    .to("openai:embeddings?embeddingModel=nomic-embed-text")
    .log("Similarity score: ${header.CamelOpenAISimilarityScore}");
----

You can also use `SimilarityUtils` directly for manual calculations:

[source,java]
----
import org.apache.camel.component.openai.SimilarityUtils;

double similarity = SimilarityUtils.cosineSimilarity(embedding1, embedding2);
double distance = SimilarityUtils.euclideanDistance(embedding1, embedding2);
List<Float> normalized = SimilarityUtils.normalize(embedding);
----

=== Embeddings Output Headers

The following headers are set after an embeddings request:

[cols="1,1,3"]
|===
| Header | Type | Description

| `CamelOpenAIEmbeddingResponseModel` | String | The model used for embedding
| `CamelOpenAIEmbeddingCount` | Integer | Number of embeddings returned
| `CamelOpenAIEmbeddingVectorSize` | Integer | Dimension of each embedding vector
| `CamelOpenAIPromptTokens` | Integer | Tokens used in the input
| `CamelOpenAITotalTokens` | Integer | Total tokens used
| `CamelOpenAIOriginalText` | String/List | Original input text(s)
| `CamelOpenAISimilarityScore` | Double | Cosine similarity (if reference embedding provided)
|===

== MCP Tool Calling (Agentic Loop)

The component supports automatic tool calling via the https://modelcontextprotocol.io[Model Context Protocol (MCP)]. When MCP servers are configured, the component acts as an MCP client: it lists available tools, converts them to OpenAI function-calling format, and runs an agentic loop — the model requests tool calls, the component executes them via MCP, feeds results back, and repeats until the model produces a final text answer.

=== MCP Server Configuration

MCP servers are configured inline on the endpoint URI using the `mcpServer.` prefix pattern. Each server is identified by a name, with sub-properties for transport type, command/URL, and arguments.

==== Streamable HTTP Transport

[source,java]
----
from("direct:chat")
    .to("openai:chat-completion?model=gpt-4"
        + "&mcpServer.api.transportType=streamableHttp"
        + "&mcpServer.api.url=http://localhost:9090/mcp");
----

==== SSE Transport

[source,java]
----
from("direct:chat")
    .to("openai:chat-completion?model=gpt-4"
        + "&mcpServer.weather.transportType=sse"
        + "&mcpServer.weather.url=http://localhost:8080");
----

==== Stdio Transport

[source,java]
----
from("direct:chat")
    .to("openai:chat-completion?model=gpt-4"
        + "&mcpServer.fs.transportType=stdio"
        + "&mcpServer.fs.command=npx"
        + "&mcpServer.fs.args=-y,@modelcontextprotocol/server-filesystem,/tmp");
----

==== Multiple MCP Servers

Multiple servers can be configured on the same endpoint. Tools from all servers are merged and made available to the model:

[tabs]
====
Java::
+
[source,java]
----
from("direct:chat")
    .to("openai:chat-completion?model=gpt-4"
        + "&mcpServer.fs.transportType=stdio"
        + "&mcpServer.fs.command=npx"
        + "&mcpServer.fs.args=-y,@modelcontextprotocol/server-filesystem,/tmp"
        + "&mcpServer.weather.transportType=sse"
        + "&mcpServer.weather.url=http://localhost:8080");
----

YAML::
+
[source,yaml]
----
- route:
    from:
      uri: direct:chat
      steps:
        - to:
            uri: openai:chat-completion
            parameters:
              model: gpt-4
              mcpServer.fs.transportType: stdio
              mcpServer.fs.command: npx
              mcpServer.fs.args: "-y,@modelcontextprotocol/server-filesystem,/tmp"
              mcpServer.weather.transportType: sse
              mcpServer.weather.url: http://localhost:8080
        - log: "${body}"
----
====

=== Agentic Loop Behavior

When the model responds with tool calls, the component automatically:

1. Executes each tool call via the corresponding MCP server
2. Sends the tool results back to the model
3. Repeats until the model produces a final text response

The `maxToolIterations` option (default: 50) prevents infinite loops. If exceeded, an `IllegalStateException` is thrown.

Set `autoToolExecution=false` to disable the agentic loop and receive raw tool calls in the message body instead:

[source,java]
----
from("direct:chat")
    .to("openai:chat-completion?model=gpt-4"
        + "&autoToolExecution=false"
        + "&mcpServer.api.transportType=streamableHttp"
        + "&mcpServer.api.url=http://localhost:9090/mcp")
    .log("Tool calls: ${body}"); // body is the raw tool calls list
----

=== Manual Tool Loop with `tool-execution` Operation

When `autoToolExecution=false`, you can implement your own tool loop using the `openai:tool-execution` operation and Camel's `loopDoWhile` EIP. This gives you full control to add logging, filtering, retry logic, or custom routing between tool calls — without writing any Java code.

The `tool-execution` operation:

* Reads the stored `ChatCompletion` response (requires `storeFullResponse=true` on the chat-completion call)
* Extracts tool calls and executes them via MCP
* Rebuilds the conversation history with the proper message chain
* Clears the body for the next chat-completion call

[tabs]
====
Java::
+
[source,java]
----
from("direct:chat")
    // Save the original prompt for the tool-execution operation
    .setProperty("originalPrompt", body())

    // Initial call: tools are listed but not auto-executed
    .to("openai:chat-completion?autoToolExecution=false"
        + "&conversationMemory=true&storeFullResponse=true"
        + "&mcpServer.api.transportType=streamableHttp"
        + "&mcpServer.api.url=http://localhost:9090/mcp")

    // Loop while the model requests tool calls
    .loopDoWhile(header("CamelOpenAIFinishReason").isEqualTo("tool_calls"))
        // Execute tool calls via MCP
        .to("openai:tool-execution"
            + "?mcpServer.api.transportType=streamableHttp"
            + "&mcpServer.api.url=http://localhost:9090/mcp")
        // Send updated conversation back to the model
        .to("openai:chat-completion?autoToolExecution=false"
            + "&conversationMemory=true&storeFullResponse=true"
            + "&mcpServer.api.transportType=streamableHttp"
            + "&mcpServer.api.url=http://localhost:9090/mcp")
    .end()

    .log("Final answer: ${body}");
----

YAML::
+
[source,yaml]
----
- route:
    from:
      uri: direct:chat
      steps:
        - setProperty:
            name: originalPrompt
            simple: "${body}"
        - to:
            uri: openai:chat-completion
            parameters:
              autoToolExecution: false
              conversationMemory: true
              storeFullResponse: true
              mcpServer.api.transportType: streamableHttp
              mcpServer.api.url: http://localhost:9090/mcp
        - loopDoWhile:
            simple: "${header.CamelOpenAIFinishReason} == 'tool_calls'"
            steps:
              - to:
                  uri: openai:tool-execution
                  parameters:
                    mcpServer.api.transportType: streamableHttp
                    mcpServer.api.url: http://localhost:9090/mcp
              - to:
                  uri: openai:chat-completion
                  parameters:
                    autoToolExecution: false
                    conversationMemory: true
                    storeFullResponse: true
                    mcpServer.api.transportType: streamableHttp
                    mcpServer.api.url: http://localhost:9090/mcp
        - log: "Final answer: ${body}"
----
====

NOTE: The `tool-execution` operation requires the `originalPrompt` exchange property (set via `setProperty` before the first call) and the `CamelOpenAIResponse` exchange property (set by `storeFullResponse=true`).

=== returnDirect

MCP tools can declare `returnDirect=true` in their annotations. When *all* tools invoked in a single batch carry this flag, the component short-circuits: it returns the tool result directly as the exchange body without sending it back to the model for further processing.

This is useful for tools whose output is the definitive answer (e.g., a database lookup) and does not need LLM interpretation.

The `CamelOpenAIMcpReturnDirect` header is set to `true` when this occurs, so downstream processors can distinguish tool-direct responses from LLM-generated ones.

NOTE: Tool execution errors always bypass `returnDirect` — errors are sent back to the model for graceful handling.

=== MCP Tool Call Headers

The following headers are set after the agentic loop completes:

[cols="1,1,3"]
|===
| Header | Type | Description

| `CamelOpenAIToolIterations` | Integer | Number of tool call iterations performed
| `CamelOpenAIMcpToolCalls` | List<String> | Ordered list of tool names called during the loop
| `CamelOpenAIMcpReturnDirect` | Boolean | `true` if the response came directly from a tool with `returnDirect`
|===

=== Conversation Memory with MCP Tools

When `conversationMemory=true`, the full tool call chain is stored in the conversation history exchange property (`CamelOpenAIConversationHistory`). This includes:

- Assistant messages containing tool call requests
- Tool result messages with execution outputs
- The final assistant text response

This enables multi-turn agentic conversations where the model can reference previous tool interactions across exchanges.

==== Multi-Turn Example

[tabs]
====
Java::
+
[source,java]
----
from("direct:chat")
    .to("openai:chat-completion?conversationMemory=true"
        + "&mcpServer.api.transportType=streamableHttp"
        + "&mcpServer.api.url=http://localhost:9090/mcp")
    .to("mock:response");
----

YAML::
+
[source,yaml]
----
- route:
    from:
      uri: direct:chat
      steps:
        - to:
            uri: openai:chat-completion
            parameters:
              conversationMemory: true
              mcpServer.api.transportType: streamableHttp
              mcpServer.api.url: http://localhost:9090/mcp
        - log: "${body}"
----
====

With this route, a multi-turn conversation works as follows:

[source,java]
----
// Turn 1: the model calls the "add" tool and returns the result
Exchange turn1 = template.request("direct:chat", e ->
    e.getIn().setBody("Use the add tool to add 15 and 27"));
// Response: "The result of adding 15 and 27 is 42."

// Turn 2: carry forward the conversation history
List<?> history = turn1.getProperty("CamelOpenAIConversationHistory", List.class);
Exchange turn2 = template.request("direct:chat", e -> {
    e.getIn().setBody("What numbers did you just add?");
    e.setProperty("CamelOpenAIConversationHistory", history);
});
// Response: "I added 15 and 27." — the model remembers the tool interaction
----

The conversation history includes the full tool call chain from turn 1 (the assistant's tool call request, the tool result, and the final answer), so in turn 2 the model has complete context of what happened — including which tools were called and what they returned.

NOTE: When `systemMessage` is set and `conversationMemory` is enabled, the conversation history is reset. This allows starting fresh conversations within the same route.

=== MCP Protocol Version

When using the Streamable HTTP transport, the component advertises MCP protocol versions during initialization. By default, the SDK's built-in versions are used. If your MCP server does not support the latest protocol version, you can restrict the advertised versions:

[source,java]
----
from("direct:chat")
    .to("openai:chat-completion?model=gpt-4"
        + "&mcpServer.api.transportType=streamableHttp"
        + "&mcpServer.api.url=http://localhost:9090/mcp"
        + "&mcpProtocolVersions=2024-11-05,2025-03-26,2025-06-18");
----

=== MCP Connection Recovery

When `mcpReconnect=true` (the default), the component automatically recovers from MCP server connection failures. If a tool call fails with a transport error, the component:

1. Closes the failed connection
2. Creates a new transport and client using the original server configuration
3. Re-initializes and re-lists available tools
4. Retries the tool call once on the new connection

This handles scenarios where an MCP server restarts, a network connection drops, or a stdio subprocess dies. If reconnection fails, the original transport error is propagated.

Set `mcpReconnect=false` to disable automatic recovery:

[source,java]
----
from("direct:chat")
    .to("openai:chat-completion?model=gpt-4"
        + "&mcpReconnect=false"
        + "&mcpServer.api.transportType=streamableHttp"
        + "&mcpServer.api.url=http://localhost:9090/mcp");
----

=== Error Handling in the Agentic Loop

[cols="1,3"]
|===
| Scenario | Behavior

| MCP client initialization failure | Route fails to start (`RuntimeException` during `doStart()`)
| Tool execution throws an exception | Error is caught, logged as WARN, and sent as tool result text to the model
| MCP transport error (`mcpReconnect=true`) | Automatic reconnection and retry (once). If retry fails, error is sent to the model
| MCP `CallToolResult.isError()` is true | Error content is sent as tool result text to the model
| Tool name not found in any server | `IllegalStateException` is thrown
| Max iterations exceeded | `IllegalStateException` is thrown with the tool call log
| Streaming + MCP tools with autoToolExecution | Falls back to non-streaming (logged as INFO)
|===

== Error Handling

The component may throw the following exceptions:

* `IllegalArgumentException`:
  ** When an invalid operation is specified (supported: `chat-completion`, `embeddings`)
  ** When message body or user message is missing
  ** When image file is provided without userMessage (chat-completion)
  ** When unsupported file type is provided (only text and image files are supported)
  ** When invalid JSON schema string is provided
* API-specific exceptions from the OpenAI SDK for network errors, authentication failures, rate limiting, etc.

include::spring-boot:partial$starter.adoc[]
